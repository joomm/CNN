{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1D7oyglJAFYwUB7BLQdmFIaYdQT_zrBvN",
      "authorship_tag": "ABX9TyMTS1OjtyTBrFyOcAsJvddR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joomm/CNN/blob/main/PL%20CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw0F4dtoLwyK",
        "outputId": "25a0f3e9-5a5b-430d-d540-a47767f9a527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "# 구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 랜덤 시드 고정\n",
        "torch.manual_seed(777)\n",
        "\n",
        "# GPU 사용 가능일 경우 랜덤 시드 고정\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 드라이브에 있는 압축 데이터 파일 풀기\n",
        "\n",
        "%cd /content/drive/MyDrive\n",
        "\n",
        "!unzip -qq \"/content/drive/MyDrive/kdata/archive.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53xyeeS3V3ZS",
        "outputId": "bfb4d217-f8c8-4d36-8fce-dac5fcca3829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "replace Plants_2/images to predict/0001_0170.JPG? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import glob\n",
        "\n",
        "images = glob.glob('/content/drive/MyDrive/Plants_2/train/*.jpg')\n",
        "\n",
        "filepaths = list(images)\n",
        "\n",
        "len(filepaths)"
      ],
      "metadata": {
        "id": "8rV-pGhKs5US"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|인덱스에 접근할 때 데이터 불러오기|\n",
        "\n",
        "• 생성자에서 인수로 넘어온 data_dir의 파일들을 데이터로 사용하기 위해 리스트로 저장하고, 이미지를 텐서로 변환하는 transform 사용.\n",
        "\n",
        "• __getitem__로 인덱스가 넘어오면, 경로에 해당하는 이미지를 불러와 텐서로 변환해 x_data로 반환.\n",
        "• 타겟 데이터(y_data)는 파일 이름으로부터 추출\n",
        "  - 식물 이름은 P0부터 P11까지 지정\n",
        "  - 전체 데이터 세트를 0000부터 0022까지의 22개 주제 범주로 나눔\n",
        "   (0000부터 0011까지는 건강한 클래스, 0012부터 0022까지는 질병 클래스)\n",
        "\n",
        "• 이를 if else문으로 판단해 반환.\n",
        "\n"
      ],
      "metadata": {
        "id": "OkUL1wqduD-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인덱스에 접근할 때 파일 경로로부터 데이터를 불러오는 데이터셋 클래스\n",
        "# 건강한 잎인지 병든 잎인지 구별하기 위한 데이터셋\n",
        "\n",
        "class PlantDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.class_to_idx = {'Alstonia Scholaris diseased (P2a)': 0,\n",
        "                             'Alstonia Scholaris healthy (P2b)': 1,\n",
        "                             'Arjun diseased (P1a)': 2,\n",
        "                             'Arjun healthy (P1b)': 3,\n",
        "                             'Bael diseased (P4b)': 4,\n",
        "                             'Basil healthy (P8)': 5,\n",
        "                             'Chinar diseased (P11b)': 6,\n",
        "                             'Chinar healthy (P11a)': 7,\n",
        "                             'Gauva diseased (P3b)': 8,\n",
        "                             'Gauva healthy (P3a)': 9,\n",
        "                             'Jamun diseased (P5b)': 10,\n",
        "                             'Jamun healthy (P5a)': 11,\n",
        "                             'Jatropha diseased (P6b)': 12,\n",
        "                             'Jatropha healthy (P6a)': 13,\n",
        "                             'Lemon diseased (P10b)': 14,\n",
        "                             'Lemon healthy (P10a)': 15,\n",
        "                             'Mango diseased (P0b)': 16,\n",
        "                             'Mango healthy (P0a)': 17,\n",
        "                             'Pomegranate diseased (P9b)': 18,\n",
        "                             'Pomegranate healthy (P9a)': 19,\n",
        "                             'Pongamia Pinnata diseased (P7b)': 20,\n",
        "                             'Pongamia Pinnata healthy (P7a)': 21}\n",
        "        self.classes = list(self.class_to_idx.keys())\n",
        "        self.transform = transform\n",
        "\n",
        "        self.samples = []\n",
        "        for plant_folder in os.listdir(data_dir):\n",
        "            if os.path.isdir(os.path.join(data_dir, plant_folder)):\n",
        "                class_name = plant_folder\n",
        "                for file in os.listdir(os.path.join(data_dir, plant_folder)):\n",
        "                    path = os.path.join(data_dir, plant_folder, file)\n",
        "                    target = self.class_to_idx[class_name]\n",
        "                    self.samples.append((path, target))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path, target = self.samples[index]\n",
        "        sample = Image.open(path)\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)"
      ],
      "metadata": {
        "id": "jHJT8Q3pY0nY"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습에 사용할 파라미터 설정."
      ],
      "metadata": {
        "id": "L1HTVwyKTh04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100"
      ],
      "metadata": {
        "id": "FraNTFteMhQ7"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/drive/MyDrive/Plants_2/train'  # 데이터가 있는 경로\n",
        "test_path = '/content/drive/My Drive/Plants_2/test'\n",
        "\n",
        "# 데이터셋에 적용할 변환 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),  # 이미지 사이즈 설정\n",
        "    transforms.ToTensor(),  # 텐서로 변환\n",
        "])"
      ],
      "metadata": {
        "id": "oecWAcOCra7b"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋 불러오기"
      ],
      "metadata": {
        "id": "GXAdkg9CTqMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ImageFolder를 사용하여 train 데이터 불러오기\n",
        "train_dataset = PlantDataset(data_dir=train_path, transform=transform)\n",
        "\n",
        "# ImageFolder를 사용하여 test 데이터 불러오기\n",
        "test_dataset = PlantDataset(data_dir=test_path, transform=transform)"
      ],
      "metadata": {
        "id": "9RPvCwVHx8nm"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터로더를 사용하여 배치 크기를 지정"
      ],
      "metadata": {
        "id": "hFpZO_wbTvOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터로더 생성\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "id": "ZrV0gP0z1drz"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "클래스로 모델을 설계"
      ],
      "metadata": {
        "id": "qw7ZBwTHT6Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# 배치 크기 × 채널 × 높이(height) × 너비(widht)의 크기의 텐서를 선언\n",
        "inputs = torch.Tensor(100, 3, 64, 64) # 텐서의 크기 print('텐서의 크기 : {}'.format(inputs.shape))\n",
        "\n",
        "conv1 = nn.Conv2d(3, 16, 5, padding=1)\n",
        "print(conv1)\n",
        "\n",
        "conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=1)\n",
        "print(conv2)\n",
        "\n",
        "pool = nn.MaxPool2d(2)\n",
        "print(pool)\n",
        "\n",
        "# 각 레이어를 통과한 후 텐서의 크기\n",
        "out = conv1(inputs)\n",
        "print(out.shape)\n",
        "\n",
        "out = pool(out)\n",
        "print(out.shape)\n",
        "\n",
        "out = conv2(out)\n",
        "print(out.shape)\n",
        "\n",
        "out = pool(out)\n",
        "print(out.shape)\n",
        "\n",
        "\n",
        "out.size(0) # 배치 크기 100\n",
        "out.size(1) # 출력 채널의 크기 32\n",
        "out.size(2) # 높이 14\n",
        "out.size(3) # 너비 14\n",
        "\n",
        "# 첫번째 차원인 배치 차원은 그대로 두고 나머지는 펼쳐라\n",
        "out = out.view(out.size(0), -1)\n",
        "print(out.shape)\n",
        "\n",
        "fc = nn.Linear(6272, 100) # input_dim = 6,272, output_dim = 100\n",
        "out = fc(out)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "InlE_DKbMhMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0501308-1859-4768-ce7c-75b08bdc73ea"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
            "Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
            "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "torch.Size([100, 16, 62, 62])\n",
            "torch.Size([100, 16, 31, 31])\n",
            "torch.Size([100, 32, 29, 29])\n",
            "torch.Size([100, 32, 14, 14])\n",
            "torch.Size([100, 6272])\n",
            "torch.Size([100, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.keep_prob = 0.5\n",
        "\n",
        "        # L1 ImgIn shape=(?, 64, 64, 3)\n",
        "        #    Conv     -> (?, 64, 64, 16)\n",
        "        #    Pool     -> (?, 32, 32, 16)\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2)) # 이미지 크기 절반\n",
        "\n",
        "        # L2 ImgIn shape=(?, 32, 32, 16)\n",
        "        #    Conv     -> (?, 32, 32, 32)\n",
        "        #    Pool     -> (?, 16, 16, 32)\n",
        "        self.layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        # L3 ImgIn shape=(?, 16, 16, 32)\n",
        "        #    Conv     -> (?, 16, 16, 64)\n",
        "        #    Pool     -> (?, 8, 8, 64)\n",
        "        self.layer3 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1))\n",
        "\n",
        "        self.fc1 = torch.nn.Linear( 3136, 100, bias=True)\n",
        "        # 전결합층 한정으로 가중치 초기화\n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        self.layer4 = torch.nn.Sequential(\n",
        "            self.fc1,\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=1 - self.keep_prob))\n",
        "\n",
        "        # L5 Final FC 100 inputs -> 14 outputs\n",
        "        self.fc2 = torch.nn.Linear(100, 14, bias=True)\n",
        "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = out.view(out.size(0), -1)   # 전결합층을 위해서 Flatten\n",
        "        out = self.layer4(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "yoGYVbfmcJYl"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 정의"
      ],
      "metadata": {
        "id": "FkwyC-b1T-rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN 모델 정의\n",
        "model = CNN().to(device)\n",
        "\n",
        "#비용 함수와 옵티마이저를 정의\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # 비용 함수에 소프트맥스 함수 포함되어져 있음.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#총 배치의 수 출력\n",
        "total_batch = len(train_dataloader)\n",
        "print('총 배치의 수 : {}'.format(total_batch))   # 총 배치 수 42"
      ],
      "metadata": {
        "id": "a6Sb5-C-UArR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7f2e12-0a21-40ee-8a10-bff7e57a8b10"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 배치의 수 : 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "총 배치의 수는 42입니다. 그런데 배치 크기를 100으로 했으므로 결국 훈련 데이터는 총 4,200개란 의미입니다. 이제 모델을 훈련시켜보겠습니다."
      ],
      "metadata": {
        "id": "tSLfxKadanf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in train_dataloader: # 미니 배치 단위로 꺼내온다. X는 미니 배치, Y는 레이블.\n",
        "        # image is already size of (28x28), no reshape\n",
        "        # label is not one-hot encoded\n",
        "        X = X.to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "egEjd1t9apBx",
        "outputId": "4be71d32-e51d-49d7-cf90-504d9f5febed"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-169-b2eb4c6a54f7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 18 is out of bounds."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수 및 최적화 알고리즘 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "SlHv-cmp9ONS"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋에 적용할 변환 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),  # 텐서로 변환\n",
        "])\n",
        "\n",
        "# PlantDataset를 사용하여 train 데이터 불러오기\n",
        "train_dataset = PlantDataset(data_dir=train_path, transform=transform)\n",
        "\n",
        "# 클래스 확인\n",
        "classes = train_dataset.classes\n",
        "\n",
        "# 클래스 이름과 인덱스 매핑\n",
        "class_to_idx = train_dataset.class_to_idx\n",
        "\n",
        "# 클래스 이름 및 해당 인덱스 출력\n",
        "for k, v in class_to_idx.items():\n",
        "    print(f\"Class: {k}, Index: {v}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG8cEipKi03U",
        "outputId": "36278a22-56b8-4e16-ce4d-edbad0737adb"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class: Alstonia Scholaris diseased (P2a), Index: 0\n",
            "Class: Alstonia Scholaris healthy (P2b), Index: 1\n",
            "Class: Arjun diseased (P1a), Index: 2\n",
            "Class: Arjun healthy (P1b), Index: 3\n",
            "Class: Bael diseased (P4b), Index: 4\n",
            "Class: Basil healthy (P8), Index: 5\n",
            "Class: Chinar diseased (P11b), Index: 6\n",
            "Class: Chinar healthy (P11a), Index: 7\n",
            "Class: Gauva diseased (P3b), Index: 8\n",
            "Class: Gauva healthy (P3a), Index: 9\n",
            "Class: Jamun diseased (P5b), Index: 10\n",
            "Class: Jamun healthy (P5a), Index: 11\n",
            "Class: Jatropha diseased (P6b), Index: 12\n",
            "Class: Jatropha healthy (P6a), Index: 13\n",
            "Class: Lemon diseased (P10b), Index: 14\n",
            "Class: Lemon healthy (P10a), Index: 15\n",
            "Class: Mango diseased (P0b), Index: 16\n",
            "Class: Mango healthy (P0a), Index: 17\n",
            "Class: Pomegranate diseased (P9b), Index: 18\n",
            "Class: Pomegranate healthy (P9a), Index: 19\n",
            "Class: Pongamia Pinnata diseased (P7b), Index: 20\n",
            "Class: Pongamia Pinnata healthy (P7a), Index: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트"
      ],
      "metadata": {
        "id": "AggIYLC4a3nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습을 진행하지 않을 것이므로 torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    X_test = []\n",
        "    Y_test = []\n",
        "    for data, labels in test_dataloader:\n",
        "        data = data.to(device)\n",
        "        label = labels.to(device)\n",
        "        X_test.append(data)\n",
        "        Y_test.append(label)\n",
        "\n",
        "    X_test = torch.cat(X_test, dim=0)\n",
        "    Y_test = torch.cat(Y_test, dim=0)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlUFWM5Fa461",
        "outputId": "80ccf579-0ffd-42b5-a2c5-3c67a1274105"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.05999999865889549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "층을 깊게 쌓는 것이 정확도를 올려주진 않음. 효율적으로 쌓는 것도 중요."
      ],
      "metadata": {
        "id": "WWZ42Zaka7kx"
      }
    }
  ]
}